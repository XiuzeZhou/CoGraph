{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "\n",
    "from utils import *\n",
    "from evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, noise_level=0.):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size, self.hidden_dim, self.noise_level = input_size, hidden_dim, noise_level\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.input_size)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.fc1(x)\n",
    "        h1 = F.relu(x)\n",
    "        return h1\n",
    "    \n",
    "    def mask(self, x):\n",
    "        corrupted_x = x + self.noise_level * torch.randn_like(x)\n",
    "        return corrupted_x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        h2 = self.fc2(x)\n",
    "        return h2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.mask(x)\n",
    "        encode = self.encoder(out)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, features, hidden, classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(features, hidden)  # shape（输入的节点特征维度 * 中间隐藏层的维度）\n",
    "        self.conv2 = GCNConv(hidden, classes)  # shape（中间隐藏层的维度 * 节点类别）\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x) \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)# 输出每一节点的类别分布\n",
    "    \n",
    "    \n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, feature, hidden, classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.sage1 = SAGEConv(feature, hidden)\n",
    "        self.sage2 = SAGEConv(hidden, classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.sage1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.sage2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, features, hidden, classes, heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(features, hidden, heads=heads)\n",
    "        self.gat2 = GATConv(hidden*heads, classes) \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(torch.nn.Module):\n",
    "    def __init__(self, data, output_size=8):\n",
    "        super(Graph, self).__init__()\n",
    "        self.x, self.edge_index = data.x, data.edge_index\n",
    "        input_size = data.x.shape[1]\n",
    "        hidden = int(input_size/2)\n",
    "        self.sage1 = SAGEConv(input_size, hidden)\n",
    "        self.sage2 = SAGEConv(hidden, output_size)\n",
    "        self.embeddings = torch.zeros((self.x.shape[0], output_size), dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            out = self.sage1(self.x, self.edge_index)\n",
    "            out = F.relu(out)\n",
    "            out = F.dropout(out, training=self.training)\n",
    "            self.embeddings = self.sage2(out, self.edge_index)\n",
    "\n",
    "        return self.embeddings[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoGa(torch.nn.Module):\n",
    "    def __init__(self, rating_mat, data_u, data_i, embedding_size, hidden_dim, num_layers=2, nhead=8, dropout=0.0):\n",
    "        super(CoGa, self).__init__()\n",
    "        N, M = rating_mat.shape\n",
    "        self.rating_mat = torch.from_numpy(rating_mat).float()\n",
    "        self.graph_u, self.graph_i = Graph(data_u, embedding_size), Graph(data_i, embedding_size)\n",
    "        self.ae_u, self.ae_i = Autoencoder(input_size=M, hidden_dim=embedding_size), Autoencoder(input_size=N, hidden_dim=embedding_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=2*embedding_size, nhead=nhead, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.cell = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(2*2*embedding_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_ids, item_ids = x[:,0], x[:,1]\n",
    "        users = self.rating_mat[user_ids]\n",
    "        items = self.rating_mat.t()[item_ids]\n",
    "        ae_users, y_users = self.ae_u(users)\n",
    "        ae_items, y_items = self.ae_i(items)\n",
    "        graph_users, graph_items = self.graph_u(user_ids), self.graph_i(item_ids)\n",
    "        embed_users, embed_items = torch.cat([ae_users, graph_users], 1), torch.cat([ae_items, graph_items], 1)\n",
    "        out = torch.cat([embed_users, embed_items], 1).reshape(-1, 2, 2*embedding_size)\n",
    "        out = self.cell(out)\n",
    "        out = out.reshape(-1, 2*2*embedding_size)\n",
    "        out = self.linear(out)\n",
    "        return out, users, y_users, items, y_items\n",
    "        \n",
    "\n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            x = np.c_[pairs[0][i],pairs[1][i]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            out, _, _, _, _ = self.forward(x)\n",
    "            predictions[i] = out.reshape(-1).data.numpy()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_mat(rating_mat):\n",
    "    '''rating_mat: user-item interactoins'''\n",
    "    N, M = rating_mat.shape\n",
    "    co_u, co_i = np.zeros((N, N)), np.zeros((M, M))\n",
    "    \n",
    "    # obtain co-occurrence of users\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            co_u[i,j] = np.sum(rating_mat[i,:]*rating_mat[j,:])\n",
    "            co_u[j,i] = co_u[i,j]\n",
    "    \n",
    "    # obtain co-occurrence of items\n",
    "    for i in range(M):\n",
    "        for j in range(i+1, M):\n",
    "            co_i[i,j] = np.sum(rating_mat[:,i]*rating_mat[:,j])\n",
    "            co_i[j,i] = co_i[i,j]\n",
    "    \n",
    "    return co_u, co_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMI_score(mat, k=2, is_normalised=False, eps=1e-20):\n",
    "    '''mat: user-item interactoins'''\n",
    "    N, M = mat.shape\n",
    "    PMI_u, PMI_i = np.zeros((N, N)), np.zeros((M, M))\n",
    "    NPMI_u, NPMI_i = np.zeros((N, N)), np.zeros((M, M))\n",
    "    items, users = np.sum(mat,0), np.sum(mat,1)\n",
    "    C_u, C_i = 0, 0   # the number of the co-occurrence user/item\n",
    "    \n",
    "    # obtain co-occurrence weights for users\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            num_i, num_j = users[i], users[j]  # the number of the user i/j makes rates\n",
    "            num_co = np.sum(mat[i,:]*mat[j,:]) # the number of users (i,j) make corate\n",
    "            C_u += num_co\n",
    "            PMI_u[i,j] = (num_co if num_co > 0 else eps) / (num_i * num_j if num_i * num_j > 0 else eps)\n",
    "            PMI_u[j,i] = PMI_u[i,j]\n",
    "    for i in range(N): PMI_u[i,i] = 1/C_u\n",
    "    PMI_u = np.log2(PMI_u*C_u)\n",
    "    \n",
    "    # obtain co-occurrence weights for items\n",
    "    for i in range(M):\n",
    "        for j in range(i+1, M):\n",
    "            num_i, num_j = items[i], items[j]  # the number of the item i/j rated by all users\n",
    "            num_co = np.sum(mat[:,i]*mat[:,j]) # the number of items (i,j) corated by all users\n",
    "            C_i += num_co\n",
    "            PMI_i[i,j] = (num_co if num_co > 0 else eps) / (num_i * num_j if num_i * num_j > 0 else eps)\n",
    "            PMI_i[j,i] = PMI_i[i,j]\n",
    "    for i in range(M): PMI_i[i,i] = 1/C_i\n",
    "    PMI_i = np.log2(PMI_i*C_i)\n",
    "    \n",
    "    PMI_u, PMI_i = PMI_u*(PMI_u > np.log2(k)), PMI_i*(PMI_i > np.log2(k))\n",
    "    \n",
    "    # NPMI\n",
    "    if is_normalised:\n",
    "        for i in range(N):\n",
    "            for j in range(i+1, N):\n",
    "                num_co = np.sum(mat[i,:]*mat[j,:]) # the number of users (i,j) make corate\n",
    "                NPMI_u[i,j] = PMI_u[i,j]/(-np.log2(num_co/C_u if num_co>0 else eps))\n",
    "                NPMI_u[j,i] = NPMI_u[i,j]\n",
    "\n",
    "        for i in range(M):\n",
    "            for j in range(i+1, M):\n",
    "                num_co = np.sum(mat[:,i]*mat[:,j]) # the number of items (i,j) corated by all users\n",
    "                NPMI_i[i,j] = PMI_i[i,j]/(-np.log2(num_co/C_i if num_co>0 else eps))\n",
    "                NPMI_i[j,i] = NPMI_i[i,j]\n",
    "                \n",
    "        return NPMI_u, NPMI_i\n",
    "    else:\n",
    "        return PMI_u, PMI_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边的连接信息\n",
    "# 注意，无向图的边要定义两次\n",
    "edge_index = torch.tensor(\n",
    "    [\n",
    "        # 这里表示节点0和1有连接，因为是无向图\n",
    "        # 那么1和0也有连接\n",
    "        # 上下对应着看\n",
    "        [0, 1, 1, 2],\n",
    "        [1, 0, 2, 1],\n",
    "    ],\n",
    "    # 指定数据类型\n",
    "    dtype=torch.long\n",
    ")\n",
    "# 节点的属性信息\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        # 三个节点\n",
    "        # 每个节点的特征向量维度为1\n",
    "        [-1],\n",
    "        [0],\n",
    "        [1],\n",
    "    ]\n",
    ")\n",
    "edge_attr = \n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=943, #item=1682, #train_pairs=99057, #test_pairs=943\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/100k'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' \n",
    "      % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(943, 1682)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "rating_mat = train_mat.todense()\n",
    "mat = 1 * (rating_mat > 0)\n",
    "print(type(mat) is np.matrix)\n",
    "print(mat.shape)\n",
    "print(mat)\n",
    "print(mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml-100k'\n",
    "data_dir = 'datasets/' + dataset + '/u.data'        # raw user-item records from the official website\n",
    "N, M, data_list = load_data(file_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(943, 1682)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "rating_mat = sequence2mat(data_list, N, M)\n",
    "mat = 1 * (rating_mat > 0)\n",
    "print(type(mat) is np.ndarray)\n",
    "print(mat.shape)\n",
    "print(mat)\n",
    "print(mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_u, co_i = co_mat(mat.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  3.,  9., 17.,  1.,  7.,  4.,  5., 19., 15., 16., 22., 10.,\n",
       "       13.,  4., 28.,  3., 18., 14., 22., 12.,  4., 18., 18., 17.,  8.,\n",
       "       14., 15.,  5.,  1., 15.,  1.,  6.,  6.,  5., 16.,  4., 22., 16.,\n",
       "       12.,  4., 15.,  5., 12.,  2.,  8.,  8., 16.,  5.,  5.,  0., 10.,\n",
       "       21.,  4., 17., 11.,  4.,  1., 30.,  5.,  8.,  6.,  8.,  6., 10.,\n",
       "       22.,  1.,  8., 20.,  1., 19.,  9., 15.,  0.,  1., 24., 15.,  4.,\n",
       "        1.,  2., 15., 10.,  6., 22.,  2., 12.,  4.,  8., 16.,  3., 21.,\n",
       "       12.,  9., 14.,  5.,  9., 11., 16., 19.,  5., 19.,  5., 19., 11.,\n",
       "        4., 14.,  2.,  4.,  2.,  6., 19., 13.,  1.,  2., 25.,  8., 12.,\n",
       "        5., 19., 13., 11., 20.,  6.,  9.,  5.,  6., 19., 12.,  2., 20.,\n",
       "        4.,  7.,  0.,  7.,  3.,  1.,  9., 17., 11.,  8.,  8.,  7.,  3.,\n",
       "       12., 16.,  9.,  4.,  4.,  3.,  6.,  4., 10.,  0., 12., 14.,  0.,\n",
       "        4.,  4., 11.,  6.,  0.,  4., 21.,  3.,  3.,  5.,  0., 12.,  4.,\n",
       "       16.,  3.,  1.,  0.,  4.,  1.,  2.,  3.,  2.,  7.,  7.,  3., 15.,\n",
       "       12.,  3., 10.,  4.,  9.,  2.,  7.,  4.,  2.,  4.,  8.,  8.,  7.,\n",
       "        7.,  2.,  0.,  3.,  3.,  8.,  7.,  7.,  2.,  0., 18.,  6., 12.,\n",
       "        0., 10.,  6.,  2.,  2.,  4.,  1.,  6.,  5.,  3.,  3., 11.,  4.,\n",
       "        8.,  1.,  5.,  6.,  5., 13.,  1.,  7., 11.,  3.,  2.,  1.,  6.,\n",
       "        4.,  3.,  1.,  0.,  3.,  2.,  1.,  5.,  0., 11.,  1.,  2.,  8.,\n",
       "        4., 13.,  3.,  1.,  3.,  7.,  6., 10.,  1.,  0.,  4.,  1.,  6.,\n",
       "        4.,  6.,  4.,  3., 16.,  4.,  2.,  0.,  2.,  1.,  8.,  1.,  3.,\n",
       "        5.,  1.,  3.,  3.,  2.,  4.,  3.,  5., 12.,  4.,  3.,  1., 15.,\n",
       "        4.,  3.,  1.,  0.,  5.,  1.,  1.,  3.,  7.,  9.,  3.,  3.,  4.,\n",
       "        3.,  4.,  1.,  2.,  6.,  4.,  4.,  1.,  2.,  5.,  8., 11., 25.,\n",
       "        3.,  6.,  1., 14.,  1.,  6.,  7.,  4.,  3.,  2.,  1.,  3.,  3.,\n",
       "       21.,  2.,  4.,  3., 19.,  2.,  0.,  5., 14.,  3., 19.,  3.,  0.,\n",
       "        0., 16., 16., 20.,  5.,  9.,  2.,  1.,  3.,  3., 25.,  2.,  2.,\n",
       "        2., 13., 13.,  3.,  0.,  3.,  1., 13.,  8.,  4.,  0.,  0.,  5.,\n",
       "       14.,  7., 14.,  1.,  3., 28.,  0.,  1.,  0.,  1., 18.,  8.,  2.,\n",
       "       15.,  2., 19.,  2.,  0.,  2.,  0., 19.,  5.,  3., 18.,  9.,  4.,\n",
       "        8.,  4., 14.,  4., 13.,  1., 13.,  6.,  1.,  2.,  2.,  3., 18.,\n",
       "        3., 20., 17., 11.,  3.,  5., 26.,  2.,  3.,  4., 23.,  4.,  5.,\n",
       "        4.,  5.,  5.,  1.,  3., 22.,  4.,  1.,  1.,  2.,  5.,  3., 16.,\n",
       "        4.,  3., 10.,  2.,  3., 20.,  4.,  2.,  3.,  3.,  4., 11.,  6.,\n",
       "       25.,  4.,  4.,  5., 13., 13.,  8.,  1., 11.,  4.,  4., 19., 10.,\n",
       "       11.,  1.,  7.,  6.,  3.,  5., 17.,  3.,  9.,  6., 12.,  3., 18.,\n",
       "        5.,  4., 11.,  9.,  6.,  4.,  3.,  1., 15., 11., 14., 15.,  3.,\n",
       "       11.,  4.,  6.,  2.,  3.,  4.,  3.,  4.,  8.,  3., 18., 11.,  8.,\n",
       "        9.,  4., 19.,  0., 16., 15.,  4., 13.,  3.,  3., 13.,  7.,  1.,\n",
       "        3.,  1.,  1.,  5., 17.,  2.,  0.,  1.,  1., 15.,  8.,  6., 12.,\n",
       "        4.,  1.,  5., 10.,  2.,  1.,  3.,  3.,  8.,  3., 18., 14.,  5.,\n",
       "        4., 21.,  2.,  8.,  7., 11.,  8., 10.,  2., 13.,  6.,  1.,  1.,\n",
       "       17.,  1.,  5.,  7.,  8., 13.,  1.,  3.,  7.,  7., 18.,  4., 12.,\n",
       "        3.,  4.,  1.,  3.,  1.,  1.,  5.,  8.,  5.,  3., 10., 17.,  5.,\n",
       "        7.,  2.,  2.,  2., 16.,  3.,  3.,  9.,  4.,  4., 19.,  2., 22.,\n",
       "        3.,  6., 16.,  3.,  2.,  2.,  2.,  2.,  3.,  3.,  0.,  8.,  1.,\n",
       "        1.,  7.,  1.,  5., 15., 11.,  2.,  4.,  4.,  3., 12.,  4.,  2.,\n",
       "        8.,  2.,  4.,  0.,  1.,  0., 11., 10.,  0., 12.,  2.,  2.,  6.,\n",
       "        5., 11., 10.,  2.,  5.,  1., 12., 17.,  7., 14.,  7.,  9.,  1.,\n",
       "       14.,  2.,  3.,  2.,  1.,  1.,  6., 18.,  4., 13.,  6., 14., 13.,\n",
       "       31.,  1.,  3.,  3., 12., 11.,  2., 11., 11.,  4., 10., 19.,  6.,\n",
       "        3.,  3.,  2.,  5.,  4.,  2.,  4.,  6.,  3.,  2.,  2., 18.,  6.,\n",
       "       13.,  3.,  4.,  2.,  2., 10., 14.,  3.,  1.,  4.,  0.,  6.,  8.,\n",
       "        1.,  6.,  7.,  8.,  4., 10.,  7., 20.,  2.,  3.,  3.,  5.,  5.,\n",
       "        2., 22.,  6., 11.,  8.,  3., 17.,  1.,  4.,  4.,  0., 12.,  2.,\n",
       "        7.,  2.,  2.,  5.,  3., 16.,  2.,  1.,  4.,  0.,  3.,  5.,  3.,\n",
       "        8.,  2.,  3.,  3.,  4.,  1.,  2.,  7.,  4.,  4.,  6., 22.,  1.,\n",
       "        1., 16.,  6.,  2.,  7.,  3.,  9., 15.,  5.,  4.,  4., 23.,  2.,\n",
       "        4.,  3.,  2.,  4., 11., 10.,  1.,  4.,  3.,  9.,  9., 10.,  4.,\n",
       "        0.,  8., 10.,  5.,  0.,  5.,  1.,  1.,  3.,  3.,  3.,  7., 11.,\n",
       "        3.,  3.,  5.,  4., 16.,  3.,  2.,  1.,  8.,  1., 16., 12.,  1.,\n",
       "       12.,  3.,  1.,  2., 15.,  5.,  2.,  4.,  3., 17.,  7.,  7.,  4.,\n",
       "        0.,  2.,  7.,  0.,  3.,  2.,  0.,  1., 16.,  7.,  2.,  7.,  1.,\n",
       "        3.,  0.,  2.,  4.,  8.,  2.,  6., 13.,  4.,  0.,  8.,  5., 14.,\n",
       "        2.,  2.,  5.,  7.,  6., 19.,  7.,  0.,  3., 15., 12.,  2.,  2.,\n",
       "        8.,  0., 15.,  5.,  2.,  2.,  0.,  1.,  6.,  2.,  6.,  8.,  1.,\n",
       "        1.,  6., 12.,  1., 21.,  4.,  9.,  3.,  6., 15.,  6.,  6.,  2.,\n",
       "        2.,  4., 11.,  2., 20., 14.,  7., 16.,  2., 19.,  9.,  6., 14.,\n",
       "        9., 18.,  6., 14.,  6.,  3., 11.,  5., 16., 15.,  8.,  8., 12.,\n",
       "        6.,  5., 10.,  1.,  3.,  2.,  1.,  3.,  0.,  3., 15.,  6.,  3.,\n",
       "        3.,  7., 12., 11.,  4.,  8.,  1.,  9.,  4.,  4.,  8.,  6.,  0.,\n",
       "        8.,  3.,  8.,  3., 16., 11.,  2.,  7., 13.,  6.,  0.,  1.,  5.,\n",
       "        3.,  7.,  3., 15.,  9.,  2.,  2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_u, PMI_i = PMI_score(mat, k=2, is_normalised=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , 12.7699641 , 13.87848855, 13.90120863, 13.38663546,\n",
       "       13.52723379, 13.18500159, 13.4305788 , 14.1620752 , 13.89435496,\n",
       "       13.85058256, 13.99807017, 13.27560414, 14.36460915, 12.5690122 ,\n",
       "       13.59705524, 13.36278871, 13.62582312, 13.30646511, 13.26110457,\n",
       "       13.79368016, 13.60003909, 13.74920554, 13.24549336, 13.67708532,\n",
       "       13.78970031, 13.76103097, 15.07629534, 13.12360105, 13.12360105,\n",
       "       13.51216634, 12.95367605, 13.06470736, 14.31624613, 14.3580663 ,\n",
       "       12.94369196, 15.18500159, 13.15118688, 13.05392752, 13.7699641 ,\n",
       "       14.1539747 , 13.02299711, 14.07629534, 13.68619574, 14.38663546,\n",
       "       13.37764667, 13.62110071, 13.61580641, 13.7637051 , 14.17251065,\n",
       "       -0.        , 14.74508943, 13.52345215, 12.61053147, 14.0749272 ,\n",
       "       13.65229533, 12.75436724, 12.12360105, 13.30257119, 14.3580663 ,\n",
       "       13.00812383, 13.5656056 , 13.99431803, 12.74663159, 14.69064164,\n",
       "       13.56808233, 13.18500159, 13.3956806 , 13.4305788 , 13.18500159,\n",
       "       13.36030131, 13.34710709, 14.11560826, -0.        , 12.31624613,\n",
       "       13.53197482, 13.3116734 , 13.4047828 , 11.92720384, 13.35101155,\n",
       "       13.38663546, 13.99706864, 13.36278871, 14.0490541 , 13.18500159,\n",
       "       13.79368016, 13.60003909, 13.65328112, 14.44177701, 13.70856355,\n",
       "       13.45702478, 13.7699641 , 13.64008381, 14.60902788, 14.33005193,\n",
       "       13.48617113, 13.84606707, 14.240958  , 14.1685885 , 13.9430288 ,\n",
       "       13.62110071, 14.50692969, 13.78656606, 14.19071525, 13.49911019,\n",
       "       13.76772562, 14.24913193, 14.62110071, 13.95367605, 15.97159796,\n",
       "       13.59893906, 13.95779216, 12.85058256, 13.46063604, 14.23867757,\n",
       "       13.96709656, 12.83000168, 12.33700469, 13.86902822, 13.86210881,\n",
       "       14.46755545, 14.18500159, 13.75436724, 15.07113363, 14.17251065,\n",
       "       14.27115824, 14.44869642, 14.01740165, 13.03613821, 13.7683968 ,\n",
       "       13.68619574, 13.38663546, -0.        , 13.28709978, 14.38663546,\n",
       "       13.38663546, 13.84506555, 15.18131655, 13.55328533, 13.92720384,\n",
       "       13.70856355, 15.38663546, 13.410883  , 14.48617113, 14.12360105,\n",
       "       13.95962531, 14.03613821, 13.13870794, 14.48617113, 14.5656056 ,\n",
       "       13.95367605, 13.59586342, -0.        , 14.05512131, 14.31624613,\n",
       "       -0.        , 14.03613821, 13.53863855, 14.05947071, 14.48617113,\n",
       "       -0.        , 13.64247436, 14.12933742, 13.7699641 , 14.59308633,\n",
       "       13.49133283, -0.        , 14.49911019, 15.12360105, 14.02206302,\n",
       "       14.53863855, 13.31624613, -0.        , 12.8630735 , 12.2822988 ,\n",
       "       11.22474777, 12.410883  , 14.06470736, 14.53863855, 13.56172216,\n",
       "       12.35101155, 13.74508943, 13.64966986, 13.70856355, 13.71760869,\n",
       "       13.2822988 , 14.55656046, 13.1539747 , 14.68302846, 15.12360105,\n",
       "       13.70856355, 13.90120863, 13.54869221, 14.66416943, 13.32609391,\n",
       "       13.96132962, 12.73128363, -0.        , 14.48617113, 14.38663546,\n",
       "       15.12360105, 15.60902788, 13.46063604, 13.21671045, -0.        ,\n",
       "       13.8672613 , 14.06470736, 14.5656056 , -0.        , 15.24913193,\n",
       "       13.9897453 , 14.24913193, 14.24913193, 15.31624613, 11.55881643,\n",
       "       14.5656056 , 15.22313672, 14.83409443, 13.64966986, 16.16799517,\n",
       "       14.2822988 , 15.12360105, 12.70856355, 13.88074453, 13.48617113,\n",
       "       13.98609753, 13.20933092, 12.80167296, 15.47152435, 15.21379886,\n",
       "       14.83409443, 12.90120863, 13.38663546, 13.55205907, 14.85058256,\n",
       "       14.20606321, 12.49911019, -0.        , 12.55205907, 12.12360105,\n",
       "       12.66416943, 15.38663546, -0.        , 14.92006766, 13.31624613,\n",
       "       12.8259205 , 14.62110071, 14.90120863, 13.37558027, 14.48617113,\n",
       "       13.12360105, 14.12360105, 14.53863855, 14.29352605, 13.22959175,\n",
       "       12.46063604, -0.        , 15.24913193, 13.38663546, 14.97159796,\n",
       "       15.38663546, 14.53863855, 13.70856355, 12.93597405, 13.77192561,\n",
       "       14.57928053, 13.31624613, -0.        , 14.00812383, 11.95367605,\n",
       "       14.85058256, 13.31624613, 14.64966986, 15.63817422, 13.06470736,\n",
       "       14.97159796, 14.7699641 , 14.38663546, 14.46063604, 14.7699641 ,\n",
       "       14.63817422, 14.85058256, 14.53863855, 13.06470736, 12.85058256,\n",
       "       14.17251065, 14.2822988 , 14.59308633, 12.95367605, -0.        ,\n",
       "       14.98609753, 12.95367605, 13.31624613, 14.83409443, 15.81547875,\n",
       "       13.91270427, 14.7699641 , 14.00812383, 14.85058256, 14.7699641 ,\n",
       "       13.23283012, 12.80167296, 13.49911019, 15.53863855, 14.03613821,\n",
       "       14.57928053, 13.18500159, 14.31624613, 12.88074453, 13.44177701,\n",
       "       13.62110071, 14.20267262, 13.5656056 , 12.14886781, 12.03613821,\n",
       "       14.31624613, 13.06470736, 14.16424303, 13.00021863, 13.68619574,\n",
       "       14.7699641 , 14.12360105, 13.38663546, 14.7699641 , 14.59308633,\n",
       "       13.72150261, 14.24913193, 13.66416943, 14.83409443, 14.93412325,\n",
       "       14.24913193, -0.        , 13.80167296, 13.52723379, 14.97159796,\n",
       "       14.30543937, 13.83409443, -0.        , -0.        , 13.8198203 ,\n",
       "       14.13870794, 14.17251065, 12.43803461, 13.24186393, 14.24913193,\n",
       "       13.31624613, 13.93597405, 13.43554506, 14.4882336 , 13.24913193,\n",
       "       13.2822988 , 13.31624613, 14.73657793, 13.1282325 , 14.59308633,\n",
       "       -0.        , 13.04559854, 13.12360105, 14.49014003, 14.85058256,\n",
       "       14.06470736, -0.        , -0.        , 14.3580663 , 13.65173233,\n",
       "       14.28709978, 13.41788639, 11.66416943, 14.20606321, 13.96517169,\n",
       "       -0.        , 12.66416943, -0.        , 12.95367605, 14.26377871,\n",
       "       13.36871355, 11.71987886, 13.48102783, 12.9806431 , 13.70856355,\n",
       "       14.38663546, -0.        , 13.00812383, -0.        , 13.87434202,\n",
       "       13.88074453, 14.33932974, 13.07113363, 14.08407269, 14.80167296,\n",
       "       13.48939503, 12.78970031, 14.08965372, 13.03613821, 13.09159066,\n",
       "       11.95367605, 14.15161543, 14.20606321, 13.24913193, 12.87567354,\n",
       "       14.06470736, 13.64966986, 12.35296774, 14.83409443, 13.61263913,\n",
       "       13.97584743, 13.50265925, 13.83409443, 14.38663546, 13.46355943,\n",
       "       13.95367605, 14.48617113, 13.85058256, 13.72037285, 14.03613821,\n",
       "       14.86056664, 13.10865071, 12.3580663 , 15.03049165, 13.12360105,\n",
       "       13.29352605, 13.47450821, 13.92720384, 12.75436724, 13.38663546,\n",
       "       13.24913193, 14.07629534, 13.33932974, 13.14250951, 14.35101155,\n",
       "       12.63531457, 13.85058256, 12.68619574, 13.83409443, 13.99157266,\n",
       "       14.66416943, 14.31624613, 12.13365471, 13.62110071, 12.63174795,\n",
       "       14.0490541 , 14.06470736, 13.27560414, 14.53863855, 14.75436724,\n",
       "       13.4157818 , 13.72250274, 13.52636022, 13.42316133, 13.06470736,\n",
       "       13.60575275, 15.12360105, 14.46063604, 13.8427489 , 13.26894041,\n",
       "       13.65229533, 12.66416943, 13.63327542, 14.20606321, 14.7699641 ,\n",
       "       14.57106003, 14.63615506, 12.63531457, 12.83956956, 13.97159796,\n",
       "       14.23824362, 14.33932974, 13.52534173, 14.60422689, 13.9806431 ,\n",
       "       14.79295574, 14.05830959, 15.16424303, 13.87567354, 13.38663546,\n",
       "       13.38663546, 13.95724266, 14.0490541 , 13.95367605, 13.7637051 ,\n",
       "       14.59308633, 15.36064025, 13.92720384, 13.21671045, 12.87567354,\n",
       "       12.52534173, 13.8259205 , 12.2822988 , 14.1539747 , 12.97385393,\n",
       "       14.16424303, 13.75436724, 13.90120863, 13.48939503, 14.20606321,\n",
       "       14.66416943, 14.14270987, -0.        , 14.38663546, 13.64391059,\n",
       "       12.90120863, 13.49014003, 14.70856355, 13.43554506, 13.80909043,\n",
       "       14.07297498, 13.12360105, 13.97159796, 13.31624613, 13.24913193,\n",
       "       15.70856355, 15.1521702 , 12.51873899, -0.        , 13.31624613,\n",
       "       12.80167296, 13.3580663 , 14.51873899, 14.53863855, 14.06470736,\n",
       "       15.18500159, 12.09385371, 14.30257119, 12.93245956, 13.21671045,\n",
       "       12.80167296, 13.8672613 , 14.08407269, 13.61053147, 14.59308633,\n",
       "       13.85612074, 13.74773415, 12.68176349, 13.62110071, 13.16424303,\n",
       "       12.38663546, 13.65328112, 14.17606847, 14.09117957, 13.36871355,\n",
       "       15.22313672, 14.18500159, 13.78695145, 13.01740165, 11.8259205 ,\n",
       "       12.80167296, 13.4123221 , 12.75436724, 13.38663546, 14.12360105,\n",
       "       14.73128363, 14.57611325, 12.38663546, 13.83409443, 14.26799096,\n",
       "       13.85770699, 13.39870829, 14.80167296, 14.07435753, 13.5656056 ,\n",
       "       15.38663546, 12.62110071, 14.16424303, 12.03613821, 13.06470736,\n",
       "       12.75436724, 14.57928053, 13.86056664, 14.12360105, 13.47590279,\n",
       "       15.58657303, 14.50692969, 14.81547875, 14.38663546, 14.00812383,\n",
       "       14.12360105, 13.89478236, 12.67881621, 12.95367605, 13.50344912,\n",
       "       13.38663546, 13.75436724, 15.56417364, 13.90120863, 13.67614207,\n",
       "       13.7389372 , 14.70856355, 14.42316133, 14.53863855, 14.38663546,\n",
       "       14.24913193, 13.1539747 , 13.2822988 , 14.64966986, 13.93597405,\n",
       "       -0.        , 13.49911019, 12.85058256, 11.23283012, 14.02406538,\n",
       "       12.1539747 , 13.44552914, 13.7328111 , 13.81926702, 13.49911019,\n",
       "       13.47974486, 14.31624613, 12.53863855, 13.53863855, 14.42316133,\n",
       "       13.90120863, 14.02206302, 13.2822988 , 12.92720384, -0.        ,\n",
       "       11.23283012, -0.        , 13.33510516, 13.61263913, -0.        ,\n",
       "       14.1539747 , 13.95367605, 11.20871766, 14.80167296, 15.27560414,\n",
       "       14.44007471, 14.11162841, 12.85058256, 13.1478486 , 13.38663546,\n",
       "       14.21671045, 14.58657303, 13.76103097, 13.20303552, 13.81547875,\n",
       "       14.11030423, 12.70856355, 13.82941795, 14.06470736, 13.22743686,\n",
       "       13.62110071, 12.31624613, 13.38663546, 13.36278871, 13.66416943,\n",
       "       13.85058256, 13.1282325 , 15.90120863, 14.31624613, 13.26434503,\n",
       "       13.24279968, 13.12360105, 13.14377893, 14.70856355, 13.48617113,\n",
       "       13.59056634, 13.46063604, 13.86421442, 13.79295574, 14.85058256,\n",
       "       13.88074453, 14.01985313, 13.37466281, 14.7699641 , 13.7389372 ,\n",
       "       13.38663546, 14.50692969, 14.57928053, 11.75436724, 13.06470736,\n",
       "       14.02673951, 13.93597405, 14.24913193, 13.09385371, 13.23824362,\n",
       "       14.33932974, 14.98273851, 14.97159796, 13.47974486, 13.70856355,\n",
       "       13.85058256, 15.70856355, 14.67042842, 14.12360105, 11.55881643,\n",
       "       12.44177701, -0.        , 13.57928053, 13.71987886, 12.18500159,\n",
       "       12.9989053 , 15.26799096, 13.87567354, 14.66416943, 13.81132313,\n",
       "       14.02406538, 14.1478486 , 14.31624613, 14.90120863, 14.04559854,\n",
       "       14.9430288 , 13.31624613, 11.60003909, 14.38009261, 13.8672613 ,\n",
       "       13.82814517, 13.32485926, 14.38663546, 13.72456403, 13.24913193,\n",
       "       14.90120863, 13.64247436, -0.        , 13.90120863, 13.24913193,\n",
       "       13.97675966, 13.42316133, 13.85058256, 14.63817422, 12.88413512,\n",
       "       13.37764667, 14.06470736, 12.85058256, 14.80167296, -0.        ,\n",
       "       14.70856355, 15.33005193, 13.7389372 , 13.49911019, 14.12360105,\n",
       "       13.29352605, 14.04559854, 14.57928053, 13.00812383, 13.66416943,\n",
       "       13.77445149, 13.80167296, 15.38663546, 14.5656056 , 13.97323831,\n",
       "       13.24913193, 12.57928053, 13.45589812, 13.53863855, 12.64247436,\n",
       "       14.14087904, 14.24913193, 14.17804883, 14.22313672, 13.26230732,\n",
       "       12.33352412, 13.51873899, 13.75234524, 14.38663546, 15.31624613,\n",
       "       14.53863855, 13.62110071, 13.87567354, 14.15676791, 14.26230732,\n",
       "       12.49911019, 14.85058256, 14.04559854, 14.74920554, 14.87848855,\n",
       "       13.69957477, 15.18500159, -0.        , 13.25735244, 13.22313672,\n",
       "       14.67293964, -0.        , 14.86056664, 12.70856355, 10.99431803,\n",
       "       13.51216634, 14.08407269, 13.27115824, 12.65793748, 14.29763045,\n",
       "       14.00812383, 13.29352605, 12.07048971, 14.66416943, 13.85681451,\n",
       "       13.46063604, 13.80167296, 12.95367605, 15.42316133, 12.31624613,\n",
       "       13.22474777, 13.9806431 , 11.92720384, 13.39265924, 14.64966986,\n",
       "       11.47974486, 13.90120863, 13.24041472, 14.98609753, 13.90120863,\n",
       "       14.1539747 , 14.70856355, 13.70326925, 13.38663546, 12.84349313,\n",
       "       15.00812383, -0.        , 14.38663546, 13.00021863, -0.        ,\n",
       "       13.33932974, 14.31624613, -0.        , 13.38663546, 14.17718209,\n",
       "       13.33600938, 13.90120863, 14.51591847, 13.31624613, 12.5656056 ,\n",
       "       -0.        , 12.51873899, 14.90120863, 14.00812383, 14.18500159,\n",
       "       13.62110071, 13.34830734, 14.06470736, -0.        , 14.16940474,\n",
       "       14.19760163, 13.89386665, 14.06470736, 14.00812383, 13.69064164,\n",
       "       12.84349313, 14.53863855, 13.29471297, 14.99235652, -0.        ,\n",
       "       14.53863855, 14.42562959, 14.06470736, 14.06470736, 13.09385371,\n",
       "       13.00120442, -0.        , 13.85390291, 14.3580663 , 14.31624613,\n",
       "       13.35101155, -0.        , 13.38663546, 15.33932974, 13.75436724,\n",
       "       15.00812383, 13.34224134, 13.00812383, 13.31624613, 13.55205907,\n",
       "       15.14377893, 13.24913193, 13.90120863, 13.66416943, 13.17804883,\n",
       "       12.72367044, 14.8672613 , 13.54399178, 13.448036  , 13.81779262,\n",
       "       14.31624613, 12.55881643, 15.38663546, 14.82814517, 14.38663546,\n",
       "       13.50692969, 14.44982928, 14.96132962, 13.69175526, 13.85058256,\n",
       "       13.89040187, 13.78045647, 14.8672613 , 13.60902788, 14.20606321,\n",
       "       13.5297604 , 15.20606321, 13.69573951, 13.39870829, 13.410883  ,\n",
       "       13.75014265, 14.47590279, 13.77192561, 13.11560826, 13.17718209,\n",
       "       13.75436724, 14.21671045, 13.20606321, 14.47590279, 13.8308193 ,\n",
       "       12.38663546, 13.67881621, 13.80167296, 13.38663546, 13.93597405,\n",
       "       -0.        , 13.08407269, 13.30711512, 13.67881621, 13.5656056 ,\n",
       "       14.7699641 , 13.82941795, 13.53197482, 14.38663546, 13.80167296,\n",
       "       13.66416943, 13.00812383, 13.88980387, 13.49911019, 14.70856355,\n",
       "       13.80167296, 13.93597405, -0.        , 14.77782621, 14.16424303,\n",
       "       12.79567421, 14.59308633, 14.26562005, 13.64443321, 13.42316133,\n",
       "       13.76103097, 14.66753628, 15.97159796, -0.        , 11.4047828 ,\n",
       "       14.70856355, 14.97159796, 13.12360105, 13.67881621, 14.46570703,\n",
       "       14.90120863, 14.00812383, 14.24913193])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPMI_u, NPMI_i = PMI_score(mat, k=4, is_normalised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.82128912 0.54555187 ... 0.70463701 0.70463701 0.70463701]\n"
     ]
    }
   ],
   "source": [
    "print(NPMI_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index(mat):\n",
    "    N, M = mat.shape\n",
    "    node0, node1 = [],[]\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            if (mat[i,j] > 0) and (i != j):\n",
    "                node0.append(i)\n",
    "                node1.append(j)\n",
    "    edge_index  = torch.tensor([node0, node1], dtype=torch.long)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_u, edge_index_i = get_edge_index(NPMI_u), get_edge_index(NPMI_i)\n",
    "x_u, x_i = torch.tensor(mat, dtype=torch.float), torch.tensor(mat.T, dtype=torch.float)\n",
    "data_u, data_i = Data(x=x_u, edge_index=edge_index_u), Data(x=x_i, edge_index=edge_index_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 8\n",
    "hidden_dim = 16\n",
    "epochs = 10\n",
    "alpha = 0.1\n",
    "batch_size = 1024\n",
    "mode = 'hr'\n",
    "topK = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CoGa(rating_mat, data_u, data_i, embedding_size, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.MSELoss() #nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print('epoch: ', epoch)\n",
    "    data_sequence = generate_instances(mat, positive_size=1, negative_time=4, is_sparse=False)\n",
    "    data_array = np.array(data_sequence)\n",
    "    x = torch.from_numpy(data_array[:,:2]).long()\n",
    "    y = torch.from_numpy(data_array[:,-1]).reshape(-1,1)\n",
    "    dataset = TensorDataset(x, y)\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    for x, y in data_loader:\n",
    "        print(batch_size)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_, u_, y_u, i_, y_i  = model(x)\n",
    "        loss = criterion(y.float(), y_.float()) + alpha * (criterion(u_.float(), y_u.float()) + criterion(i_.float(), y_i.float()))\n",
    "        optimizer.zero_grad()              # clear gradients for this training step\n",
    "        loss.backward()                    # backpropagation, compute gradients\n",
    "        optimizer.step()                   # apply gradients\n",
    "    \n",
    "    # Evaluation\n",
    "    hr, ndcg = evaluate(model, test_ratings, test_negatives, topK)\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)\n",
    "    print('epoch=%d, loss=%.4f, HR=%.4f, NDCG=%.4f' %(epoch, loss, hr, ndcg))\n",
    "\n",
    "    mlist = hr_list\n",
    "    if mode == 'ndcg':\n",
    "        mlist = ndcg_list\n",
    "    if (len(mlist) > 10) and (mlist[-2] < mlist[-3] > mlist[-1]):\n",
    "        best_hr, best_ndcg = hr_list[-3], ndcg_list[-3]\n",
    "        break\n",
    "    best_hr, best_ndcg = hr, ndcg\n",
    "\n",
    "print(\"End. Best HR = %.4f, NDCG = %.4f. \" %(best_hr, best_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss() #nn.CrossEntropyLoss()\n",
    "aa = torch.tensor([0.1,1,0,0], dtype=torch.float)\n",
    "bb = torch.tensor([0.1,0,1,1], dtype=torch.float)\n",
    "criterion(aa,bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
